{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/apal9569/PPO/blob/main/PPO_Implementation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dSnP6dcB9UwC"
      },
      "outputs": [],
      "source": [
        "import copy\n",
        "import gym\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch import optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torch import nn\n",
        "from torch.distributions import Normal\n",
        "from collections import deque\n",
        "import torch.nn.functional as F\n",
        "from IPython.display import clear_output\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import random\n",
        "\n",
        "batch_size = 50\n",
        "epochs = 1000\n",
        "learning_rate = 1e-2\n",
        "hidden_size = 8\n",
        "n_layers = 2\n",
        "gamma = 0.99\n",
        "\n",
        "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class PolModel(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(PolModel, self).__init__()\n",
        "\n",
        "        self.fc1 = nn.Linear(in_dim, 64)\n",
        "        self.fc2 = nn.Linear(64,64)\n",
        "        self.mu = nn.Linear(64, 1)\n",
        "        self.std = nn.Linear(64,1)\n",
        "\n",
        "        self.tanh  = nn.Tanh()\n",
        "        self.relu = nn.LeakyReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        mu = self.tanh(self.mu(x))\n",
        "        std = torch.exp(self.tanh(self.std(x)))\n",
        "        dist = Normal(mu, std)\n",
        "  \n",
        "        return dist\n",
        "    \n",
        "    def sample_act(self,state):\n",
        "        dist = self.forward(state)\n",
        "        action = dist.sample()\n",
        "        return dist, torch.clamp(action, -2., 2.)\n",
        "    \n",
        "class ValModel(nn.Module):\n",
        "    def __init__(self, in_dim):\n",
        "        super(ValModel, self).__init__()\n",
        "        self.fc1 = nn.Linear(in_dim, 64)\n",
        "        self.fc2 = nn.Linear(64, 1)\n",
        "        self.relu = nn.ReLU()\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "    \n",
        "class History:\n",
        "    def __init__(self):\n",
        "        self.states, self.actions, self.rewards, self.log_probs, self.values, self.dones = [], [], [], [], [], []\n",
        "    \n",
        "    def append(self, state, action, log_prob, value, reward, done):\n",
        "        self.states.append(state)\n",
        "        self.actions.append(action)\n",
        "        self.log_probs.append(log_prob)\n",
        "        self.values.append(value)\n",
        "        self.rewards.append(reward)\n",
        "        self.dones.append(done)\n",
        "\n",
        "    def clear(self):\n",
        "        self.states.clear()\n",
        "        self.actions.clear()\n",
        "        self.rewards.clear()\n",
        "        self.dones.clear()\n",
        "        self.log_probs.clear()\n",
        "        self.values.clear()\n"
      ],
      "metadata": {
        "id": "805HJExU9b3Z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "941d8394-2bc2-43f7-9ac1-74da452d79ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/ipykernel/ipkernel.py:283: DeprecationWarning: `should_run_async` will not call `transform_cell` automatically in the future. Please pass the result to `transformed_cell` argument and any exception that happen during thetransform in `preprocessing_exc_tuple` in IPython 7.17 and above.\n",
            "  and should_run_async(code)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"Pendulum-v1\")\n",
        "gamma = 0.9\n",
        "epsilon = 0.2\n",
        "\n",
        "data_collections = 1000\n",
        "train_epoch = 30\n",
        "obs_dim = 3\n"
      ],
      "metadata": {
        "id": "4fm1LddZ9gXp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07095c49-80d8-46a3-e8c9-e021e7ab70c8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/gym/core.py:317: DeprecationWarning: \u001b[33mWARN: Initializing wrapper in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n",
            "/usr/local/lib/python3.10/dist-packages/gym/wrappers/step_api_compatibility.py:39: DeprecationWarning: \u001b[33mWARN: Initializing environment in old step API which returns one bool instead of two. It is recommended to set `new_step_api=True` to use new step API. This will be the default behaviour in future.\u001b[0m\n",
            "  deprecation(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def dataCollection():\n",
        "    score = 0\n",
        "    state = np.reshape(env.reset(), (1, -1))\n",
        "\n",
        "    for _ in range(data_collections):\n",
        "        state = torch.FloatTensor(state).to(device)\n",
        "        dist, action = polModel.sample_act(state)\n",
        "\n",
        "        value = valModel(state)\n",
        "\n",
        "        action_taken = action.detach().cpu().numpy()[0]\n",
        "        next_state, reward, done, _ = env.step(action_taken)\n",
        "\n",
        "        next_state = np.reshape(next_state, (1, -1))\n",
        "        reward = np.reshape(reward, (1, -1))\n",
        "        done = np.reshape(done, (1, -1))\n",
        "\n",
        "        history.append(state, action, dist.log_prob(action), value, torch.FloatTensor(reward).to(device), torch.FloatTensor(1 - done).to(device))\n",
        "        \n",
        "        state = next_state\n",
        "        score += reward[0][0]\n",
        "\n",
        "        if done[0][0]:\n",
        "            scores.append(score)\n",
        "            score = 0\n",
        "            state = env.reset()\n",
        "            state = np.reshape(state, (1, -1))\n",
        "    \n",
        "    value = valModel(torch.FloatTensor(next_state))\n",
        "    history.values.append(value)\n"
      ],
      "metadata": {
        "id": "dQ2jHBBZ9jfH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def trainModel(config):\n",
        "\n",
        "    Qs = []\n",
        "    togo = 0\n",
        "    ####Considered lamda = 1\n",
        "    for i in reversed(range(len(history.rewards))):\n",
        "        print(history.rewards[i], gamma, history.values[i + 1] ,history.dones[i] , history.values[i])\n",
        "        delta = (history.rewards[i] + gamma * history.values[i + 1] * history.dones[i] - history.values[i])\n",
        "        togo = delta + gamma * history.dones[i] * togo\n",
        "        Qs.insert(0, togo + history.values[i])\n",
        "        \n",
        "    actor_losses, critic_losses = [], []\n",
        "\n",
        "    states = torch.cat(history.states).view(-1, obs_dim)\n",
        "    actions = torch.cat(history.actions)\n",
        "    Qs = torch.cat(Qs).detach()\n",
        "    log_probs = torch.cat(history.log_probs).detach()\n",
        "    values = torch.cat(history.values).detach()\n",
        "    advantages = Qs - values[:-1] \n",
        "\n",
        "    for _ in range(train_epoch):\n",
        "        for batch_use in range(states.size(0) // batch_size):\n",
        "          stop = states.size(0) if batch_size * (batch_use+1) > states.size(0) else batch_size * (batch_use+1)\n",
        "          ids = np.arange(batch_size * batch_use, batch_size * (batch_use+1), 1)\n",
        "          state = states[ids, :]\n",
        "          action = actions[ids]\n",
        "          Q = Qs[ids]\n",
        "          old_log_prob = log_probs[ids]\n",
        "          advantage = advantages[ids]\n",
        "\n",
        "          dist, _  = polModel.sample_act(state)\n",
        "          cur_log_prob = dist.log_prob(action)\n",
        "          ratio = torch.exp(cur_log_prob - old_log_prob)\n",
        "\n",
        "          entropy = dist.entropy().mean()\n",
        "\n",
        "          loss =  advantage * ratio\n",
        "\n",
        "          cur_value = valModel(state)\n",
        "          critic_loss = torch.mean(torch.square(Q - cur_value))\n",
        "          critic_loss_copy = copy.deepcopy(critic_loss.clone().detach().numpy())\n",
        "\n",
        "          if config[0]:\n",
        "            if config[1]:\n",
        "              clipped_loss = (torch.clamp(ratio, 1. - epsilon, 1. + epsilon) * advantage - entropy * 0.005 - critic_loss_copy * 0.005)\n",
        "              actor_loss = -torch.mean(torch.min(loss, clipped_loss))\n",
        "            else:\n",
        "              clipped_loss = (torch.clamp(ratio, 1. - epsilon, 1. + epsilon)  - entropy * 0.005 - critic_loss_copy * 0.005)\n",
        "              actor_loss = -torch.mean(torch.min(loss, clipped_loss))\n",
        "          else:\n",
        "            if config[1]:\n",
        "              actor_loss = -torch.mean(loss) - ( - entropy * 0.005 )\n",
        "            else:\n",
        "              actor_loss = -torch.mean(ratio) - ( - entropy * 0.005 )\n",
        "          \n",
        "        \n",
        "          pol_optimizer.zero_grad()\n",
        "          actor_loss.backward()\n",
        "          pol_optimizer.step()\n",
        "\n",
        "          val_optimizer.zero_grad()\n",
        "          critic_loss.backward()\n",
        "          val_optimizer.step()\n",
        "\n",
        "          actor_losses.append(actor_loss.item())\n",
        "          critic_losses.append(critic_loss.item())\n",
        "    actor_loss = sum(actor_losses) / len(actor_losses)\n",
        "    critic_loss = sum(critic_losses) / len(critic_losses)\n",
        "    total_actor_loss.append(actor_loss)\n",
        "    total_critic_loss.append(critic_loss)\n",
        "    history.clear()\n"
      ],
      "metadata": {
        "id": "1UIqll6q9oUL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def _plot_train_history():\n",
        "    data = [scores, np.array(total_actor_loss) + np.array(total_critic_loss)]\n",
        "    labels = [\"Score\",\n",
        "              \"Total Loss\"]\n",
        "\n",
        "    fig, axes = plt.subplots(2, 1, figsize=(8, 8))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.plot(data[i])\n",
        "        ax.set_title(labels[i])\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "CtsBYixh9slO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_value_s(config):\n",
        "  theta_range = np.arange(-np.pi, np.pi, 0.1)\n",
        "  ang_vel_range = np.arange(-8, 8, 0.1)\n",
        "\n",
        "  theta_grid, ang_vel_grid = np.meshgrid(theta_range, ang_vel_range)\n",
        "  states = np.stack([np.sin(theta_grid), np.cos(theta_grid), ang_vel_grid], axis=-1)\n",
        "\n",
        "  with torch.no_grad():\n",
        "      state_values1 = valModel(torch.tensor(states, dtype=torch.float)).detach().numpy().reshape(theta_grid.shape)\n",
        "  fig, axs = plt.subplots(nrows=1, ncols=1,figsize=(6, 6))\n",
        "  m1 = axs.imshow(state_values1, extent=[-np.pi, np.pi, -8, 8], cmap='jet', aspect='auto')\n",
        "  axs.set_title('Clipped ' + str(config[0]) + ' and Advantage ' + str(config[1]))\n",
        "  axs.set(xlabel='Theta', ylabel='Angular velocity')\n",
        "  plt.colorbar(m1, ax=axs)\n",
        "  plt.show()\n"
      ],
      "metadata": {
        "id": "crO2YXi79vKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_9pXjCCt9xG5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################### KEEEP THIS FOR EQ-9 ################################\n",
        "## I ran this part w.r.t to the loss function mentioned in Eq 9\n",
        "for config in [[True, True], [True, False], [False, True], [False, False]]:\n",
        "  print(config)\n",
        "  polModel = PolModel(obs_dim).to(device)\n",
        "  valModel = ValModel(obs_dim).to(device)\n",
        "  pol_optimizer = optim.Adam(polModel.parameters(), lr=1e-3)\n",
        "  val_optimizer = optim.Adam(valModel.parameters(), lr=5e-3)\n",
        "  history = History()\n",
        "  actor_loss_history = []\n",
        "  critic_loss_history = []\n",
        "  scores = []\n",
        "\n",
        "  for step in range(epochs):\n",
        "    dataCollection()\n",
        "    trainModel(config)\n",
        "    \n",
        "  _plot_train_history()\n",
        "  plot_value_s(config)"
      ],
      "metadata": {
        "id": "rh9Px-g3NLOZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MCptEuDvCtty"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "########################### KEEP THIS FOR EQ - 7 #######################################\n",
        "## I ran this part w.r.t to the loss function mentioned in Eq 9\n",
        "\n",
        "for config in [[True, True], [True, False], [False, True], [False, False]]:\n",
        "  print(config)\n",
        "  polModel = PolModel(obs_dim).to(device)\n",
        "  valModel = ValModel(obs_dim).to(device)\n",
        "  pol_optimizer = optim.Adam(polModel.parameters(), lr=1e-3)\n",
        "  val_optimizer = optim.Adam(valModel.parameters(), lr=5e-3)\n",
        "  history = History()\n",
        "  actor_loss_history = []\n",
        "  critic_loss_history = []\n",
        "  scores = []\n",
        "\n",
        "  for step in range(epochs):\n",
        "    dataCollection()\n",
        "    trainModel(config)\n",
        "    \n",
        "  _plot_train_history()\n",
        "  plot_value_s(config)"
      ],
      "metadata": {
        "id": "1hiEWsVzNLQf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XldgoFZUHYks"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}